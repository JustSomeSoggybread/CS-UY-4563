{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network from Scratch\n",
    "The following neural network code was adapted from https://deeplearning.lipingyang.org/wp-content/uploads/2016/12/Neural-Networks-Tutorial-A-Pathway-to-Deep-Learning-Adventures-in-Machine-Learning.pdf.\n",
    "\n",
    "The notation that was used in this website is almost the same as the notation we are using in class.  Instead of using $a$ for activations the author uses $h$, and instead of $N$ for the number of training examples, the author uses $m$. (I have modified the code below to use $a$ and $N$.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Your assignment:\n",
    "\n",
    "Modify the neural network implementation below (it is the same implementation that we discussed in class) to see if you can improve the performance by trying the following:\n",
    "\n",
    "\n",
    "* Add a regularization term to the cost function. \n",
    "$$ J(W,b)\n",
    "= \\frac{1}{N}\n",
    "{\\Big [}\\sum_{i=1}^N\n",
    " J(W,b,{\\bf x}^{(i)},y^{(i)}){\\Big ]}\n",
    "+ \\frac{\\lambda}{2}\\sum_{\\ell}\\sum_{i}\\sum_j (W^{(\\ell)}_{ij})^2$$\n",
    "\n",
    "The partial derivative would be: \n",
    "$$\\frac{\\partial J(W,b)}{\\partial W^{(\\ell)}_{ij}}\n",
    "= \\frac{1}{N}\n",
    "{\\Big [}\\sum_{i=1}^N\n",
    "\\frac{\\partial J(W,b,{\\bf x}^{(i)},y^{(i)})}{\\partial W^{(\\ell)}_{ij}} {\\Big ]}\n",
    "+ {\\lambda} W^{(\\ell)}_{ij}$$ where ${\\bf x}^{(i)},y^{(i)}$ are the $i$th training example.  <br><br>\n",
    "\n",
    "* Try using the **ReLU** activation function, $f(z) = \\max(0,z)$. You will notice it is not differentiable at $0$, but you can use: $f'(z) = 0$ if $z<0$ and $f'(z)=1$ if $z\\ge 0$.  (You can also try using the **leaky ReLU** activation function.)  For more information see https://www.kaggle.com/dansbecker/rectified-linear-units-relu-in-deep-learning.  (The performance of the ReLU may surprise you.) <br><br>\n",
    "\n",
    "* Try using the **tanh** activation function, $f(z) = \\frac{e^z-e^{-z}}{e^z+e^{-z}}$.  The derivative of **tanh** is $f'(z)=1-(f(z))^2$.  For more information see http://ufldl.stanford.edu/tutorial/supervised/MultiLayerNeuralNetworks/ <br><br>\n",
    "* Try using  the other activation functions including **ELU** (exponential linear unit) $ELU_{\\alpha}(z)= \\begin{cases}  \\alpha(\\exp (z)-1) & \\text{if } z<  0 \\\\ z & \\text{if } z\\ge 0 \\end{cases}$\n",
    "* Try changing the number of iterations\n",
    "* Try changing the number of hidden layers\n",
    "* Try initializing the weights using one of the techniques mentioned in the lecture\n",
    "\n",
    "\n",
    "For each of the modifications about, describe your experience (i.e. what did you try and how well did it peform).  What gave the best performance?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The first thing we will do is import all the libraries\n",
    "\n",
    "We will be using the lower resolution MINST data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits # The MNIST data set is in scikit learn data set\n",
    "from sklearn.preprocessing import StandardScaler  # It is important in neural networks to scale the date\n",
    "from sklearn.model_selection import train_test_split  # The standard - train/test to prevent overfitting and choose hyperparameters\n",
    "from sklearn.metrics import accuracy_score # \n",
    "import numpy as np\n",
    "import numpy.random as r # We will randomly initialize our weights\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at the data\n",
    "\n",
    "After we load the data, we print the shape of the data and a pixelated digit.\n",
    "\n",
    "We also show what the features of one example looks like.\n",
    "\n",
    "The neural net will learn to estimate which digit these pixels represent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the digits dataset:\n",
      "(1797, 64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 480x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "[ 0.  0.  5. 13.  9.  1.  0.  0.  0.  0. 13. 15. 10. 15.  5.  0.  0.  3.\n",
      " 15.  2.  0. 11.  8.  0.  0.  4. 12.  0.  0.  8.  8.  0.  0.  5.  8.  0.\n",
      "  0.  9.  8.  0.  0.  4. 11.  0.  1. 12.  7.  0.  0.  2. 14.  5. 10. 12.\n",
      "  0.  0.  0.  0.  6. 13. 10.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "digits=load_digits()\n",
    "X = digits.data\n",
    "print(\"The shape of the digits dataset:\") \n",
    "print(digits.data.shape)\n",
    "plt.gray()\n",
    "plt.matshow(digits.images[0])\n",
    "plt.show()\n",
    "y = digits.target\n",
    "print(y[0:1])\n",
    "print(X[0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Scale the dataset\n",
    "The training features range from 0 to 15.  To help the algorithm converge, we will scale the data to have a mean of 0 and unit variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        , -0.33501649, -0.04308102,  0.27407152, -0.66447751,\n",
       "       -0.84412939, -0.40972392, -0.12502292, -0.05907756, -0.62400926,\n",
       "        0.4829745 ,  0.75962245, -0.05842586,  1.12772113,  0.87958306,\n",
       "       -0.13043338, -0.04462507,  0.11144272,  0.89588044, -0.86066632,\n",
       "       -1.14964846,  0.51547187,  1.90596347, -0.11422184, -0.03337973,\n",
       "        0.48648928,  0.46988512, -1.49990136, -1.61406277,  0.07639777,\n",
       "        1.54181413, -0.04723238,  0.        ,  0.76465553,  0.05263019,\n",
       "       -1.44763006, -1.73666443,  0.04361588,  1.43955804,  0.        ,\n",
       "       -0.06134367,  0.8105536 ,  0.63011714, -1.12245711, -1.06623158,\n",
       "        0.66096475,  0.81845076, -0.08874162, -0.03543326,  0.74211893,\n",
       "        1.15065212, -0.86867056,  0.11012973,  0.53761116, -0.75743581,\n",
       "       -0.20978513, -0.02359646, -0.29908135,  0.08671869,  0.20829258,\n",
       "       -0.36677122, -1.14664746, -0.5056698 , -0.19600752])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_scale = StandardScaler()\n",
    "X = X_scale.fit_transform(digits.data)\n",
    "\n",
    "X[0,:] # Looking the new features after scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Creating training and test datasets\n",
    "We split the data into training and test data sets. We will train the neural network with the training dataset, and evaluate our neural network with the test dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the data into training and test set.  60% training and %40 test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Setting up the output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-vs-all encoding\n",
    "Our target is an integer in the range [0,..,9], so we will have 10 output neuron's in our network.  \n",
    "\n",
    "-  If  $y=0$, we want the output neurons to have the values $(1,0,0,0,0,0,0,0,0,0)$\n",
    "\n",
    "-  If  $y=1$ we want the output neurons to have the values $(0,1,0,0,0,0,0,0,0,0)$\n",
    "-  etc\n",
    "\n",
    "Thus we need to change our target so it is the same as our hoped for output of the neural network.  \n",
    "-  If $y=0$ we change it into the vector $(1,0,0,0,0,0,0,0,0,0)$. \n",
    "-  If $y=1$ we change it into the vector $(0,1,0,0,0,0,0,0,0,0)$\n",
    "-  etc\n",
    "\n",
    "See page 29 from the website listed above\n",
    "\n",
    "The code to covert the target vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_y_to_vect(y):\n",
    "    y_vect = np.zeros((len(y), 10))\n",
    "    for i in range(len(y)):\n",
    "        y_vect[i, y[i]] = 1\n",
    "    return y_vect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the training and test targets to vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert digits to vectors\n",
    "y_v_train = convert_y_to_vect(y_train)\n",
    "y_v_test = convert_y_to_vect(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick check to see that our code performs as we expect "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6 0 5 6]\n",
      "[[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(y_train[0:4])\n",
    "print(y_v_train[0:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Creating the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The activation function and its derivative\n",
    "\n",
    "We will use the sigmoid activation function:  $f(z)=\\frac{1}{1+e^{-z}}$\n",
    "\n",
    "The deriviative of the sigmoid function is: $f'(z) = f(z)(1-f(z))$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(z):\n",
    "    # ReLu\n",
    "    # return np.maximum(0, z) #\n",
    "    # Sigmoid:\n",
    "    # return 1 / (1 + np.exp(-z))\n",
    "    # Tanh:\n",
    "    return np.tanh(z)\n",
    "\n",
    "\n",
    "def f_deriv(z):\n",
    "    # ReLu\n",
    "    # return np.where(z > 0, 1, 0) #\n",
    "    # Sigmoid:\n",
    "    #return f(z) * (1 - f(z))\n",
    "    # Tanh:\n",
    "    return 1 - np.tanh(z) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating and initialing W and b\n",
    "We want the weights in W to be different so that during back propagation the nodes on a level will have different gradients and thus have different update values.\n",
    "\n",
    "We want the  weights to be small values, since the sigmoid is almost \"flat\" for large inputs.\n",
    "\n",
    "Next is the code that assigns each weight a number uniformly drawn from $[0.0, 1.0)$.  The code assumes that the number of neurons in each level is in the python list *nn_structure*.\n",
    "\n",
    "In the code, the weights, $W^{(\\ell)}$ and $b^{(\\ell)}$ are held in a python dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_and_init_weights(nn_structure):\n",
    "    W = {} #creating a dictionary i.e. a set of key: value pairs\n",
    "    b = {}\n",
    "    for l in range(1, len(nn_structure)):\n",
    "        W[l] = r.random_sample((nn_structure[l], nn_structure[l-1])) #Return “continuous uniform” random floats in the half-open interval [0.0, 1.0). \n",
    "        b[l] = r.random_sample((nn_structure[l],))\n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing $\\triangledown W$ and $\\triangledown b$\n",
    "Creating $\\triangledown W^{(\\ell)}$ and $\\triangledown b^{(\\ell)}$ to have the same size as $W^{(\\ell)}$ and $b^{(\\ell)}$, and setting $\\triangledown W^{(\\ell)}$, and  $\\triangledown b^{(\\ell)}$ to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_tri_values(nn_structure):\n",
    "    tri_W = {}\n",
    "    tri_b = {}\n",
    "    for l in range(1, len(nn_structure)):\n",
    "        tri_W[l] = np.zeros((nn_structure[l], nn_structure[l-1]))\n",
    "        tri_b[l] = np.zeros((nn_structure[l],))\n",
    "    return tri_W, tri_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed forward\n",
    "Perform a forward pass throught the network.  The function returns the values of $a$ and $z$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward(x, W, b):\n",
    "    a = {1: x} # create a dictionary for holding the a values for all levels\n",
    "    z = { } # create a dictionary for holding the z values for all the layers\n",
    "    for l in range(1, len(W) + 1): # for each layer\n",
    "        node_in = a[l]\n",
    "        z[l+1] = W[l].dot(node_in) + b[l]  # z^(l+1) = W^(l)*a^(l) + b^(l)\n",
    "        a[l+1] = f(z[l+1]) # a^(l+1) = f(z^(l+1))\n",
    "    return a, z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute $\\delta$\n",
    "The code below compute $\\delta^{(s_l)}$ in a function called \"calculate_out_layer_delta\",  and  computes $\\delta^{(\\ell)}$ for the hidden layers in the function called \"calculate_hidden_delta\".  \n",
    "\n",
    "If we wanted to have a different cost function, we would change the \"calculate_out_layer_delta\" function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_out_layer_delta(y, a_out, z_out):\n",
    "    # delta^(nl) = -(y_i - a_i^(nl)) * f'(z_i^(nl))\n",
    "    return -(y-a_out) * f_deriv(z_out) \n",
    "\n",
    "\n",
    "def calculate_hidden_delta(delta_plus_1, w_l, z_l):\n",
    "    # delta^(l) = (transpose(W^(l)) * delta^(l+1)) * f'(z^(l))\n",
    "    return np.dot(np.transpose(w_l), delta_plus_1) * f_deriv(z_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Back Propagation Algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn(nn_structure, X, y, iter_num=3000, alpha=0.25, lambda_reg=0.001):\n",
    "    W, b = setup_and_init_weights(nn_structure)\n",
    "    cnt = 0\n",
    "    N = len(y)\n",
    "    avg_cost_func = []\n",
    "    print('Starting gradient descent for {} iterations'.format(iter_num))\n",
    "    while cnt < iter_num:\n",
    "        if cnt%1000 == 0:\n",
    "            print('Iteration {} of {}'.format(cnt, iter_num))\n",
    "        tri_W, tri_b = init_tri_values(nn_structure)\n",
    "        avg_cost = 0\n",
    "        for i in range(N):\n",
    "            delta = {}\n",
    "            # perform the feed forward pass and return the stored a and z values, to be used in the\n",
    "            # gradient descent step\n",
    "            a, z = feed_forward(X[i, :], W, b)\n",
    "            # loop from nl-1 to 1 backpropagating the errors\n",
    "            for l in range(len(nn_structure), 0, -1):\n",
    "                if l == len(nn_structure):\n",
    "                    delta[l] = calculate_out_layer_delta(y[i,:], a[l], z[l])\n",
    "                    avg_cost += np.linalg.norm((y[i,:]-a[l]))\n",
    "                else:\n",
    "                    if l > 1:\n",
    "                        delta[l] = calculate_hidden_delta(delta[l+1], W[l], z[l])\n",
    "                    # triW^(l) = triW^(l) + delta^(l+1) * transpose(a^(l))\n",
    "                    tri_W[l] += np.dot(delta[l+1][:,np.newaxis], np.transpose(a[l][:,np.newaxis]))# np.newaxis increase the number of dimensions\n",
    "                    # trib^(l) = trib^(l) + delta^(l+1)\n",
    "                    tri_b[l] += delta[l+1]\n",
    "        # perform the gradient descent step for the weights in each layer\n",
    "        for l in range(len(nn_structure) - 1, 0, -1):\n",
    "            W[l] += -alpha * ((1.0/N * tri_W[l]) + (lambda_reg * W[l]))\n",
    "            b[l] += -alpha * (1.0/N * tri_b[l])\n",
    "        # complete the average cost calculation\n",
    "        reg_term = sum(np.sum(np.square(W[l])) for l in range(1, len(nn_structure)))\n",
    "        avg_cost = 1.0/N * avg_cost + (lambda_reg/2.0) * reg_term\n",
    "        avg_cost_func.append(avg_cost)\n",
    "        cnt += 1\n",
    "    return W, b, avg_cost_func\n",
    "\n",
    "\n",
    "def predict_y(W, b, X, n_layers):\n",
    "    N = X.shape[0]\n",
    "    y = np.zeros((N,))\n",
    "    for i in range(N):\n",
    "        a, z = feed_forward(X[i, :], W, b)\n",
    "        y[i] = np.argmax(a[n_layers])\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the neural network\n",
    "\n",
    "Our code assumes the size of each layer in our network is held in a list.  The input layer will have 64 neurons (one for each pixel in our 8 by 8 pixelated digit).  Our hidden layer has 30 neurons (you can change this value).  The output layer has 10 neurons.\n",
    "\n",
    "Next we create the python list to hold the number of neurons for each level and then run the neural network code with our training data.\n",
    "\n",
    "This code will take some time..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting gradient descent for 3000 iterations\n",
      "Iteration 0 of 3000\n",
      "Iteration 1000 of 3000\n",
      "Iteration 2000 of 3000\n"
     ]
    }
   ],
   "source": [
    "nn_structure = [64, 50, 30, 10]\n",
    "    \n",
    "# train the NN\n",
    "W, b, avg_cost_func = train_nn(nn_structure, X_train, y_v_train, 3000, 0.25, 0.003)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the learning curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the avg_cost_func\n",
    "plt.plot(avg_cost_func)\n",
    "plt.ylabel('Average J')\n",
    "plt.xlabel('Iteration number')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Assessing accuracy\n",
    "Next we determine what percentage the neural network correctly predicted the handwritten digit correctly on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction accuracy is 96.38386648122392%\n"
     ]
    }
   ],
   "source": [
    "# get the prediction accuracy and print\n",
    "y_pred = predict_y(W, b, X_test, 4)\n",
    "print('Prediction accuracy is {}%'.format(accuracy_score(y_test, y_pred) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding regularization term to cost function:\n",
    "With lambda = 0.01, this greatly increase average J with each iteration, with J ending after 3000 iterations around J=1, and also slightly decreased the prediction accuracy - to around 83%, down from 84%. This is generally expected, as adding regularization is intended to prevent overfitting, and thus may also slightly decrease accuracy.\n",
    "With lambda = 0.001, this still slightly increased average J with each iteration, with J ending around 0.6 after 3000 iterations. However, prediction accuracy actually increased, from 83% to 94%. This indicates that the algorithm was overfitting without regluarization, and so with a slight regularization term of 0.001, this overfitting is partially prevented.\n",
    "\n",
    "With lambda = 0.003, average J stayed around 0.6 after 3000 iterations, but prediction accuracy increased slightly from 94% to 95%. These diminishing returns in lambda changes indicates that an \"ideal\" lambda is somewhere around 0.03, maybe a bit higher, around 0.05.\n",
    "\n",
    "Future answers will assume a lambda of 0.03.\n",
    "\n",
    "## Using ReLu instead of sigmoid:\n",
    "\n",
    "With ReLu, average J was substantially higher. As such, prediction accuracy was substantially lower - around 10.01%, which is around expected for a blind guess. This could potentially indicate that ReLu is not well-suited for this sort of decision problem.\n",
    "\n",
    "## Using Tanh instead of sigmoid:\n",
    "With Tanh, average J dropped to around 0.3. Prediction accuracy was very high, with an accuracy of 97.7%. This is the best performing activation function yet.\n",
    "\n",
    "Future answers will assume a activation function using sigmoid if not otherwise stated.\n",
    "\n",
    "## Increasing the number of iterations\n",
    "Using 5000 iterations, average J dropped slightly, although hardly noticeable. Accuracy increased marginally, from 94.8% to 95.7%. These diminishing returns indicates that further increasing the number of iterations will only have marginal benefits relative to the increased time cost to run the algorithm.\n",
    "\n",
    "## Increasing the number of hidden layers\n",
    "Using an nn structure of [64, 50, 30, 10] for 3000 iterations, J stood round 1 and prediction accuracy stood around 10% while using a sigmoid activation function.\n",
    "However, using the tanh activation function, J stood around 0.3 and prediction accuracy stood around 96.4%."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
